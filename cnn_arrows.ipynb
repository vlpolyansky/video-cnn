{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "def randfloat(a, b):\n",
    "    return rand.random() * (b - a) + a\n",
    "\n",
    "def my_print(s):\n",
    "    sys.stdout.write(s)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"./data/arrows/\"\n",
    "save_dir = \"./save\"\n",
    "save_file_path = os.path.join(save_dir, \"model.ckpt\")\n",
    "logs_dir = \"./logs\"\n",
    "logs_dir_2 = \"./logs2\"\n",
    "\n",
    "create_dir(save_dir)\n",
    "create_dir(logs_dir)\n",
    "create_dir(logs_dir_2)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "IMAGE_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-9ded3c54d32d>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-9ded3c54d32d>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    my_print()\"Total: %i images, train: %i\\n\" % (cnt, train_cnt))\u001b[0m\n\u001b[1;37m                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def _create_label_producers():\n",
    "    labels_fname = os.path.join(data_dir, \"labels.csv\")\n",
    "    labels = pd.read_csv(labels_fname, header=None)\n",
    "    lines = [os.path.join(data_dir, \"input\", str(l[0]) + \".png\") + \" \" + str((l[1] + 1) / 2) for l in labels.values]\n",
    "    cnt = len(lines)\n",
    "    p = 0.9\n",
    "    train_cnt = int(cnt * p)\n",
    "    my_print(\"Total: %i images, train: %i\\n\" % (cnt, train_cnt))\n",
    "    train_lines = lines[:train_cnt]\n",
    "    test_lines = lines[train_cnt:]\n",
    "    return tf.train.string_input_producer(train_lines), tf.train.string_input_producer(test_lines)\n",
    "\n",
    "def _read_image_and_label(path_label_line):\n",
    "    with tf.name_scope(\"read_image_and_label\"):\n",
    "        path, label_str = tf.decode_csv(path_label_line, [[\"\"], [\"\"]], field_delim=\" \")\n",
    "        label = tf.cast(tf.string_to_number(label_str, out_type=tf.int32), tf.int64)\n",
    "        file_content = tf.read_file(path)\n",
    "        image3c = tf.image.decode_png(file_content)\n",
    "        image1, image2 = tf.split(1, 2, image3c)\n",
    "        image1 = tf.cast(image1, tf.float32)\n",
    "        image2 = tf.cast(image2, tf.float32)\n",
    "        image1.set_shape((IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "        image2.set_shape((IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    return image1, image2, label\n",
    "\n",
    "def _generate_batch(image1, image2, label, batch_size, min_after_dequeue=20000):\n",
    "    with tf.name_scope(\"generate_batch\"):\n",
    "        images1, images2, labels = tf.train.shuffle_batch([image1, image2, label], batch_size=batch_size, \n",
    "                                                capacity=min_after_dequeue + batch_size * 4, \n",
    "                                                min_after_dequeue=min_after_dequeue,\n",
    "                                                num_threads=5)\n",
    "    return images1, images2, tf.reshape(labels, [batch_size])\n",
    "    \n",
    "def get_data_batch():\n",
    "    train_queue, test_queue = _create_label_producers()\n",
    "    image1_train, image2_train, label_train = _read_image_and_label(train_queue.dequeue())\n",
    "    image1_test, image2_test, label_test = _read_image_and_label(test_queue.dequeue())\n",
    "    images1_train, images2_train, labels_train = _generate_batch(image1_train, image2_train, \n",
    "                                                                 label_train, BATCH_SIZE)\n",
    "    images1_test, images2_test, labels_test = _generate_batch(image1_test, image2_test, \n",
    "                                                              label_test, BATCH_SIZE,\n",
    "                                                              min_after_dequeue=BATCH_SIZE)\n",
    "    return images1_train, images2_train, labels_train, images1_test, images2_test, labels_test\n",
    "\n",
    "def dense_to_one_hot(label_batch, num_labels=2):\n",
    "    with tf.name_scope(\"one_hot_encoder\"):\n",
    "        sparse_labels = tf.cast(tf.reshape(label_batch, [-1, 1]), tf.int32)\n",
    "        derived_size = tf.shape(sparse_labels)[0]\n",
    "        indices = tf.reshape(tf.range(0, derived_size, 1), [-1, 1])\n",
    "        concated = tf.concat(1, [indices, sparse_labels])\n",
    "        outshape = tf.concat(0, [tf.reshape(derived_size, [1]), tf.reshape(num_labels, [1])])\n",
    "        labels = tf.sparse_to_dense(concated, outshape, 1.0, 0.0)\n",
    "        labels = tf.cast(labels, tf.float32)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _activation_summary(x):\n",
    "    tensor_name = x.op.name\n",
    "    tf.histogram_summary(tensor_name + '/activations', x)\n",
    "    tf.scalar_summary(tensor_name + '/sparsity', tf.nn.zero_fraction(x))\n",
    "\n",
    "\n",
    "def _create_conv(layer, kernel_shape, kernel_stddev=1e-3, activation=tf.nn.relu, name=\"conv\", trainable=True):\n",
    "    with tf.name_scope(name):\n",
    "        kernel = tf.Variable(tf.truncated_normal(kernel_shape, stddev=kernel_stddev), name=\"kernel\", \n",
    "                             trainable=trainable)\n",
    "        tf.add_to_collection('main', kernel)\n",
    "        conv = tf.nn.conv2d(layer, kernel, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[kernel_shape[3]]), name=\"biases\", trainable=trainable)\n",
    "        tf.add_to_collection('main', biases)\n",
    "        biased = tf.nn.bias_add(conv, biases)\n",
    "        conv_relu = activation(biased)\n",
    "        _activation_summary(conv_relu)\n",
    "\n",
    "    return conv_relu\n",
    "    \n",
    "def _create_pool(layer, kernel_width, stride=1, name=\"pool\"):\n",
    "    with tf.name_scope(name):\n",
    "        pool = tf.nn.max_pool(layer, ksize=[1, kernel_width, kernel_width, 1], strides=[1, stride, stride, 1],\n",
    "                             padding='SAME')\n",
    "    return pool\n",
    "\n",
    "def _create_norm(layer, name=\"norm\"):\n",
    "    with tf.name_scope(name):\n",
    "        norm = tf.nn.lrn(layer, alpha=0.001 / 9.0, beta=0.75)\n",
    "    return norm\n",
    "\n",
    "def _create_full(layer, result_size, weights_stddev=.04, activation=tf.nn.relu, name=\"full\", trainable=True):\n",
    "    with tf.name_scope(name):\n",
    "        layer_size = 1\n",
    "        for k in layer.get_shape()[1:].as_list():\n",
    "            layer_size *= k\n",
    "        reshaped = tf.reshape(layer, [BATCH_SIZE, layer_size])\n",
    "        weights = tf.Variable(tf.truncated_normal([layer_size, result_size], stddev=weights_stddev), name=\"weights\",\n",
    "                             trainable=trainable)\n",
    "        tf.add_to_collection('main', weights)\n",
    "        biases = tf.Variable(tf.constant(.1, shape=[result_size]), name=\"biases\", trainable=trainable)\n",
    "        tf.add_to_collection('main', biases)\n",
    "        if activation:\n",
    "            biased = tf.add(tf.matmul(reshaped, weights), biases)\n",
    "            result = activation(biased)\n",
    "        else:\n",
    "            result = tf.add(tf.matmul(reshaped, weights), biases)\n",
    "        \n",
    "        _activation_summary(result)\n",
    "    return result\n",
    "    \n",
    "def build_classifier(images1, images2, trainable=True):\n",
    "    images = tf.concat(3, [images1, images2])\n",
    "    \n",
    "    conv1 = _create_conv(images, [5, 5, 6, 64], name=\"conv1\", trainable=trainable)\n",
    "    pool1 = _create_pool(conv1, 3, 2, name=\"pool1\")\n",
    "    norm1 = _create_norm(pool1, name=\"norm1\")\n",
    "    \n",
    "    conv2 = _create_conv(norm1, [5, 5, 64, 64], name=\"conv2\", trainable=trainable)\n",
    "    pool2 = _create_pool(conv2, 3, 2, name=\"pool2\")\n",
    "    norm2 = _create_norm(pool2, name=\"norm2\")\n",
    "    \n",
    "    conv3 = _create_conv(norm2, [4, 4, 64, 32], name=\"conv3\", trainable=trainable)\n",
    "    pool3 = _create_pool(conv3, 3, 2, name=\"pool3\")\n",
    "#     norm3 = _create_norm(pool3, name=\"norm3\")\n",
    "    \n",
    "    full1 = _create_full(pool3, 256, name=\"full1\", activation=tf.nn.relu, trainable=trainable)\n",
    "    full2 = _create_full(full1, 128, name=\"full2\", activation=tf.nn.relu, trainable=trainable)\n",
    "    full3 = _create_full(full2, 2, name=\"full3\", activation=None, trainable=trainable)\n",
    "    \n",
    "    softmax = tf.nn.softmax(full3, name=\"softmax\")\n",
    "    \n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, labels):\n",
    "    with tf.name_scope(\"cross_entropy\"):\n",
    "        labels = tf.cast(labels, tf.int64)\n",
    "        dense_labels = dense_to_one_hot(labels)\n",
    "        clipped_logits = tf.clip_by_value(logits, 0.00001, 100.0)\n",
    "        cross_entropy = -dense_labels * tf.log(clipped_logits)\n",
    "        cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "\n",
    "    return cross_entropy_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_train(loss_op, step, init_rate=0.1, decay_steps=6000):\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        learning_rate = tf.train.exponential_decay(init_rate, step, decay_steps, 0.1, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        grads = optimizer.compute_gradients(loss_op)\n",
    "\n",
    "        apply_gradient_op = optimizer.apply_gradients(grads, global_step=step)\n",
    "\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.histogram_summary(var.op.name, var)\n",
    "\n",
    "        for grad, var in grads:\n",
    "            if grad:\n",
    "                tf.histogram_summary(var.op.name + '/gradients', grad)\n",
    "\n",
    "    return apply_gradient_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load(saver, sess):\n",
    "    saver.restore(sess, save_file_path)\n",
    "    my_print(\"Model restored.\\n\")\n",
    "    \n",
    "def save(saver, sess):\n",
    "    save_path = saver.save(sess, save_file_path)\n",
    "    my_print(\"Model saved in file: %s\\n\" % save_path)\n",
    "\n",
    "def train_classifier(need_load, N=10000):\n",
    "    \n",
    "    with tf.Graph().as_default() as g: \n",
    "        \n",
    "        step = tf.Variable(0, trainable=False, name=\"step\")\n",
    "        tf.add_to_collection(\"main\", step)\n",
    "    \n",
    "        images1_train, images2_train, labels_train, images1_test, images2_test, labels_test = get_data_batch()\n",
    "        images1_ph = tf.placeholder(tf.float32, [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 3], \"images1\")\n",
    "        images2_ph = tf.placeholder(tf.float32, [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 3], \"images2\")\n",
    "        labels_ph = tf.placeholder(tf.int64, [BATCH_SIZE], \"labels\")\n",
    "        answer_op = build_classifier(images1_ph, images2_ph, trainable=True)\n",
    "        loss_op = build_loss(answer_op, labels_ph)\n",
    "        train_op = build_train(loss_op, step)\n",
    "        correct_prediction = tf.equal(tf.argmax(answer_op, 1), labels_ph)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        accuracy_summary = tf.scalar_summary(\"accuracy\", accuracy)\n",
    "\n",
    "        merged_summaries = tf.merge_all_summaries()\n",
    "\n",
    "        init = tf.initialize_all_variables()\n",
    "\n",
    "        sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "        saver = tf.train.Saver(var_list=tf.get_collection(\"main\"))\n",
    "        coord = tf.train.Coordinator()\n",
    "        writer = tf.train.SummaryWriter(logs_dir, sess.graph_def, flush_secs=30)\n",
    "\n",
    "        sess.run(init)\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "        try:\n",
    "\n",
    "            if need_load:\n",
    "                load(saver, sess)\n",
    "\n",
    "            my_print(\"Starting...\\n\")\n",
    "\n",
    "            for i in xrange(0, N):\n",
    "                if i % 11 == 0:\n",
    "                    im1, im2, lab = sess.run([images1_test, images2_test, labels_test])\n",
    "                    feed = {\n",
    "                        images1_ph : im1,\n",
    "                        images2_ph : im2,\n",
    "                        labels_ph : lab\n",
    "                    }\n",
    "                    result = sess.run([merged_summaries, accuracy, step], feed_dict=feed)\n",
    "                    summary_str = result[0]\n",
    "                    acc = result[1]\n",
    "                    st = result[2]\n",
    "                    writer.add_summary(summary_str, st)\n",
    "                    print(\"Accuracy at step %s: %s\" % (st, acc))\n",
    "                else:\n",
    "                    im1, im2, lab = sess.run([images1_train, images2_train, labels_train])\n",
    "                    feed = {\n",
    "                        images1_ph : im1,\n",
    "                        images2_ph : im2,\n",
    "                        labels_ph : lab\n",
    "                    }\n",
    "                    sess.run(train_op, feed_dict=feed)\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    save(saver, sess)\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_classifier(need_load=False, N=10000)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def pair_loss(images1, images2):\n",
    "    return tf.constant(0, dtype=tf.float32)\n",
    "\n",
    "def gen_image():\n",
    "    N = 20000\n",
    "    \n",
    "    with tf.Graph().as_default() as g:\n",
    "        \n",
    "        step = tf.Variable(0, trainable=False, name=\"step_2\")\n",
    "    \n",
    "        _, _, _, images1_test, _, labels_test = get_data_batch()\n",
    "        images1_ph = tf.placeholder(tf.float32, [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 3], \"images1\")\n",
    "#         images2 = tf.Variable(tf.random_uniform([BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 3], 0, 1), \"images2\")\n",
    "        images2 = tf.Variable(tf.constant(0, tf.float32, [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 3]), \"images2\")\n",
    "#         images2 = tf.Variable(images1_ph, \"images2\")\n",
    "        tf.image_summary(\"given\", images1_ph)\n",
    "        tf.image_summary(\"generated\", images2)\n",
    "        labels_ph = tf.placeholder(tf.int64, [BATCH_SIZE], \"labels\")\n",
    "        answer_op = build_classifier(images1_ph, images2, trainable=False)\n",
    "        l_loss = build_loss(answer_op, labels_ph)\n",
    "        p_loss = pair_loss(images1_ph, images2)\n",
    "        tf.scalar_summary(\"l_loss\", l_loss)\n",
    "        tf.scalar_summary(\"p_loss\", p_loss)\n",
    "        loss_op = l_loss + p_loss\n",
    "        train_op = build_train(loss_op, step, init_rate=10., decay_steps=5000)\n",
    "        correct_prediction = tf.equal(tf.argmax(answer_op, 1), labels_ph)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        accuracy_summary = tf.scalar_summary(\"accuracy\", accuracy)\n",
    "\n",
    "        merged_summaries = tf.merge_all_summaries()\n",
    "\n",
    "        init = tf.initialize_all_variables()\n",
    "        \n",
    "        sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "        saver = tf.train.Saver(var_list=tf.get_collection(\"main\"))\n",
    "        coord = tf.train.Coordinator()\n",
    "        writer = tf.train.SummaryWriter(logs_dir_2, sess.graph_def, flush_secs=30)\n",
    "\n",
    "        sess.run(init)\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        \n",
    "        \n",
    "        im1, lab = sess.run([images1_test, labels_test])\n",
    "        feed = {\n",
    "            images1_ph : im1,\n",
    "            labels_ph : lab\n",
    "        }\n",
    "        \n",
    "        sess.run(tf.assign(images2, images1_ph), feed_dict=feed)\n",
    "\n",
    "        try:\n",
    "\n",
    "            load(saver, sess)\n",
    "\n",
    "            my_print(\"Starting...\\n\")\n",
    "\n",
    "            for i in xrange(N):\n",
    "                if i % 11 == 0:\n",
    "                    result = sess.run([merged_summaries, accuracy, step], feed_dict=feed)\n",
    "                    summary_str = result[0]\n",
    "                    acc = result[1]\n",
    "                    st = result[2]\n",
    "                    writer.add_summary(summary_str, st)\n",
    "                    print(\"Accuracy at step %s: %s\" % (st, acc))\n",
    "                else:\n",
    "                    sess.run(train_op, feed_dict=feed)\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
